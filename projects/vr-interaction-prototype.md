# üëÅÔ∏è VR Interaction Prototype

Modular VR interaction prototype built with Unity 6 using the 3d Universal Rendering Pipeline.  Exploring gaze-based interaction, accessible navigation, and early universal-design principles.

## üß† Concept

This prototype explores how VR interactions can be made intuitive, comfortable and accessible.  It combines gaze-based interaction, animated feedback, spatial audio cues, and multiple navigation modes, including locomotion and teleportation with vignettes, to reduce simulator sickness.

The name of the project is 'Reality is Blurry' and the long-term goal is to evolve this into a game that raises awareness of visual impairment and includes features that support players with low vision.

## üß© My Role
- Built gaze-based interaction using XR Interaction Toolkit
- Added animated and audio feedback to reinforce user intent
- Implemented locomtion, teleportation, and vignette-based comfort settings
- Designed interaction logic with modularity and accessibilty in mind
- Prototyped early universal-design features for low-vision players
- Debugged collider issues, timing prbolems, and intraction edge cases

## üõ†Ô∏è Tech Stack

Unity | C# | XR Interaction Toolkit | OpenXR | Meta Quest Developer Hub | Spatial Audio | Animator Controller

## üîß Process

### Goal
Create VR interaction system that feels intuitive, comfortable and accessible, whilst laying groundwork for future game that tackles visual impairment in novel ways.

### Constraints
- Must work on standard VR hardware without eye-tracking
- Needs to minimise simulator sickness
- Interactions must be obvious for players with varying visual abilities
- Prototype must reamin modular for future expansion

### Approach
- Used XR Interaction Toolkit for consistent interaction patterns
- Implemented gaze-based selection using raycasts + timed activation
- Added animation and sound cues to reinforce feedback
- Integrated locomotion & teleport with distinct vignette comfort modes
- Structured logic into modular components for easy iteration

### Key Decisions
- Feedback is multimodal (animation + sound) to support low-vision users
- Interaction scripts are modular to support future accessiblity features

## üé• Media
- gaze interaction activating the helmet

- vignette comfort mode

- diagram of gaze interaction state flow

## ‚≠ê Reflections

### What worked

### What I'd improve

### What I learned
